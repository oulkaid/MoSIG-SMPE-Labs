---
title: "TP : is Batman somewhere ?"
author: "Oussama Oulkaid"
date: "December, 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1) 
```

```{r, results=FALSE, message=FALSE}
# The environment
library(tidyverse)
library(ggplot2)
library(corrplot)
```

## Dataframe
```{r, message=FALSE}
myData <- read.table("bats.csv", sep=";", skip=3, header=T)
names(myData)
```

## 2 - Relationship between brain weight and body mass
```{r, message=FALSE}
# Focusing only on the phytophagous
phyto = myData[(myData$Diet==1),]
ggplot(myData, aes(x=BOW, y=BRW)) + geom_point(size = 0.5) +
  xlab("Body mass (g)") + ylab("Brain mass (mg)") + theme_light()
```

- We can spot an outlier element that has both a huge body and brain mass compared to the whole distribution.

- We launch the simple linear regression. The estimated model has the following form: $BRW = \hat \beta_1 + \hat \beta_2 \times BOW + \epsilon$
```{r, message=FALSE}
# Simple regression model: BRW = b1 + b2*BOW + error
#                          BRW = 623.4469 + 8.9999*BOW + error
reg1 = lm(BRW ~ BOW, data=phyto)
summary(reg1)
```
- From the summary we have the numerical form of the model: $BRW = 623.4469 + 8.9999 \times BOW + \epsilon$ (where `623.4469` is the estimate of the intercept $\hat \beta_1$).
- We also see that the value of the coefficient of determination $\hat \beta_2$ (`~9`) is significantly different from zero.
- At the end of the row we got three stars, which is the highest level of significance of the variable `BOW` in the model.
- In addition, the very low `p-value` (`< 2.2e-16`) reflects the fact that the variable `BOW` has a big influence on `BRW`. Thus, the relationship between brain weight and body mass can be estimated to be linear.
- The H0 hypothesis would be to say that the coefficient $\hat \beta_2$ is null (meaning that `BOW` has no influence of the value of `BRW`). This hypothesis is false.

- we draw the regression line:
```{r, message=FALSE}
ggplot(myData, aes(x=BOW, y=BRW)) + geom_point(size = 0.5) +
  stat_smooth(method="lm", se=TRUE) +
  xlab("Body mass (g)") + ylab("Brain mass (mg)") + theme_light()
```

- Analysis of the variance table:
```{r, message=FALSE}
anova(reg1)
```
- Additional information in the table: SSE (sum of squares of the error) and SSM (sum of squares of the model).
- The sum of squared residuals is `4253838`.

- We draw the graph of the residuals with respect to the predicted values:
```{r, message=FALSE}
plot(reg1$fitted.values, reg1$residuals, xlab="Predicted", ylab="Residuals")
```

- The graph shows that the value of residuals generally increases for higher values of predicted brain weight. At a first glance, we can assume that the outlier element (with a predicted value of brain weight around `1000`) caused a deviation in the linear regression model, because the model tries to include all the elements of the dataset.

- The Cook's distance graph shows that the seventh element in the dataset has the largest distance. It corresponds to the outlier previously cited.
```{r, message=FALSE}
plot(reg1, 4) #Cook's distance
```

- We redo the analysis without this individual and we compare the results obtained:
```{r, message=FALSE}
which(phyto$BRW>8000)
phytobis = phyto[which(phyto$BRW<8000),]
reg2 = lm(BRW ~ BOW, data=phytobis)
summary(reg2)
```
- Now, we clearly notice an improvement of the relevance of the model ; the coefficient of determination is much more significant ($\hat \beta_2$). it's value is `14.5099` (was `8.9999` for `reg1`). This goes along with a decrease of the intercept ($\hat \beta_1$).
- Thus, our assumption was valid ; removing the outlier from the linear regression model is helpful. 
- The Cook's distance corresponding to the new model is:
```{r, message=FALSE}
plot(reg2, 4) #Cook's distance
```

<!--
- Diagnosis of the validity of the linear model approach
```{r, message=FALSE}
#par(mfcol=c(2,2))
#plot(reg1)
#plot(reg2)
```
-->
<!--
- One advantage is that the second model allows to have a wider view of the distribution of the sample.
-->

## 3 - Contribution of each part of the brain to the total weight
<!--
```{r, message=FALSE}
phytoNum=phyto[,c(4:8)]
mat.cor=cor(phytoNum)
corrplot(mat.cor, type="upper")
```

-->

- Pearson tests
```{r, message=FALSE}
cor.test(phyto$BRW, phyto$HIP)
cor.test(phyto$BRW, phyto$MOB)
cor.test(phyto$BRW, phyto$AUD)
```
- between `BRW` and `HIP` : the correlation is the highest (`p-value = 4.574e-13`).
- between `BRW` and `MOB` : the correlation is high (`p-value = 2.203e-10`).
- between `BRW` and `AUD` : the correlation is lower (`p-value = 0.003215`).

- Multiple regression model: $BRW = \hat \beta_1 + \hat \beta_2 \times AUD + \hat \beta_3 \times MOB + \hat \beta_4 \times HIP + \epsilon$
```{r, message=FALSE}
# BRW = b1 + b2*AUD + b3*MOB + b4*HIP + error
# BRW = -312.692 + 47.989*AUD - 2.444*MOB + 15.981*HIP
regm = lm(BRW~AUD+MOB+HIP, data=phytobis)
summary(regm)
```
- The numerical model: $BRW = -312.692 + 47.989 \times AUD - 2.444 \times MOB + 15.981 \times HIP + \epsilon$
- The `p-value` is very small. We can tell that the variable `BRW` is influenced by the `AUD` and `HIP`. `MOB` on the other hand seems to not have a significant coefficient ($\hat \beta_3$) compared to ($\hat \beta_2$ and $\hat \beta_4$).

```{r, message=FALSE}
anova(regm)
```

- We run the following function : 
```{r, message=FALSE}
reg0 = lm(BRW ~ 1, data = phyto)
step(reg0, scope=BRW~AUD + MOB + HIP, direction="forward")
```

- The purpose of this function is to start from a linear regression model (`reg0` in this case) and perform some analysis by adding at each time a new variable. It seems that the variables are added in a specific order such that the variable with the highest `p-value` is added first, and so on.
- *"The AIC is designed to find the model that explains the most variation in the data, while penalizing for models that use an excessive number of parameters.. The lower the AIC, the better the model fit."* (source: https://www.statology.org/aic-in-r/)
- If we consider this definition, we can assume that the multivariate model (`BRW~AUD + MOB + HIP`) fits better.

## 4 - Link between volume of tthe auditory part and diet
```{r, message=FALSE}
myData$Diet_F = as.factor(myData$Diet)
with(myData, plot(AUD~Diet))
with(myData, plot(AUD~Diet_F))
```

- I think it's preferable to look at the first plot. Especially that for some diet categories the number of samples is not representative (`2` and `4`). So looking only at the factors graph would be misleading.

- Regression analysis:
```{r, message=FALSE}
lm = lm(AUD~Diet_F, data=myData)
summary(lm)
anova(lm)
```

- Conclusion : there is no correlation between the auditory brain volume and diet ; the sum of squared residuals is way more large than the sum of squares of the model. In addition, the corresponding `p-value` is relatively high.
